# Парсер и веб-скрепер, реализованные в 2024 году в рамках проекта <code>Academic Theses Analysis in Contemporary Russia</code>

Реализованный код позволяет получить из открытого доступа подробную базу данных объемом более 200.000 строк и размером более 1GB.

## Цель проекта

Построение обогащённой базы данных по академическим диссертациям для семантического анализа:

- Веб-сбор данных о диссертациях (карточки, ссылки, метаданные).
- Извлечение текста из PDF-файлов.
- Выделение сущностей (научный руководитель, официальные оппоненты, ведущая организация).
- Формирование табличного набора с текстом и выделенными полями.

---

## Данные и их смысл

- **Карточки объявлений/диссертаций** (источник: веб-страницы, например, реестр ВАК): структурированные элементы HTML со ссылками на подробные страницы и/или файлы PDF.
- **PDF диссертаций/авторефератов**: документ первичного источника с формализованными разделами, где встречаются искомые сущности (научный руководитель, оппоненты, ведущая организация).
- **Табличные файлы**:
  - `adverts.json` — сырые данные карточек, собранные с веб-страниц.
  - `database.csv` — табличное представление реестра диссертаций (как правило, содержит минимум: идентификаторы, базовые метаданные и **ссылки на PDF**).
  - `result.csv` — итоговый набор: `database.csv` + текст PDF + выделенные сущности.

---

## Логические модули и их назначение

### A. Веб-скрейпинг карточек (Selenium)
**Файлы:** `webscraper.py`, `parser.py`  
**Назначение:** Получить из веб-интерфейса набор карточек с ключевыми полями и ссылками для последующей обработки.  
**Ключевые методы:**

- Selenium WebDriver (Chrome в headless-режиме) для эмуляции действий браузера.
- WebDriverWait + expected_conditions — ожидание появления элементов перед чтением.
- CSS/XPath-селекторы для адресного извлечения полей карточек (название, дата, ссылка и т. п.).
- BeautifulSoup для доп. парсинга HTML (если требуется доочистка).
- Контроль пагинации/подгрузки результатов (пролистывание, кнопки «Ещё», вкладки).

---

### B. Нормализация и табличное представление
**Файлы:** `adverts.json` → (вне объёма демонстратора) → `database.csv`  
**Назначение:** Привести "сырые" данные к стабильной табличной форме.  
**Типовые операции:**

- Очистка пробелов/служебных символов.
- Верификация обязательных полей (например, наличие **ссылки на PDF**).
- Снятие дубликатов.

---

### C. Загрузка PDF и извлечение текста (PyMuPDF)
**Файл:** `pdf_text_extractor.py`  
**Назначение:** По ссылке из таблицы загрузить PDF и извлечь из него текст, пригодный для последующего разбора.

**Ключевые шаги:**
1. Чтение входной таблицы: `database.csv` в `pandas.DataFrame`.
2. HTTP-загрузка PDF по URL (модуль `requests`); сохранение во временный файл.
3. Текстовая экстракция с помощью `fitz` (PyMuPDF).

